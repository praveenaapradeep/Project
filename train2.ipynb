{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gKNQ8pk38U8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for data processing, modeling, and visualization\n",
        "import pandas as pd  # Data manipulation and analysis\n",
        "import numpy as np  # Numerical operations and array manipulation\n",
        "import plotly.express as plx  # Interactive data visualization\n",
        "from imblearn.over_sampling import SMOTE  # Handling imbalanced datasets through oversampling\n",
        "from sklearn.preprocessing import OneHotEncoder  # Encoding categorical features\n",
        "from sklearn.preprocessing import MinMaxScaler  # Scaling features to a specific range\n",
        "from tensorflow.keras.models import Sequential  # Building a sequential model in Keras\n",
        "from tensorflow.keras.layers import GRU, Dense  # GRU (Gated Recurrent Unit) and Dense layers for neural networks\n",
        "from tensorflow.keras.metrics import MeanAbsolutePercentageError  # MAPE metric for model evaluation\n",
        "import tensorflow as tf  # Deep learning and neural network tasks\n",
        "import matplotlib.pyplot as plt  # Data visualization and plotting\n",
        "from google.colab import drive  # Mounting Google Drive in Colab to access files\n",
        "\n",
        "# Mount Google Drive to access data files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the stock prices dataset from Google Drive\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/praveena stock/prices.csv\")\n",
        "print(data.shape)  # Display the shape of the dataset to confirm successful loading\n",
        "data.head()  # Display the first few rows of the dataset to inspect the data structure\n",
        "\n",
        "# Analyze the distribution of stock symbols in the dataset\n",
        "unique_symbols = data['symbol'].value_counts()\n",
        "print(unique_symbols)  # Print the count of each unique stock symbol\n",
        "\n",
        "# Display basic information about the dataset including data types and missing values\n",
        "data.info()\n",
        "\n",
        "# Filter the dataset for Google stocks using the stock symbol 'GOOG'\n",
        "google = data[data['symbol'] == 'GOOG']\n",
        "google.head()  # Display the first few rows of Google stock data for inspection\n",
        "print(google.shape)  # Display the shape of the filtered Google stock data\n",
        "\n",
        "# Data Visualization: Plot the difference between open and close prices for Google stocks\n",
        "plx.line(google, x=\"date\", y=[\"open\", \"close\"], title=\"Difference between open and close prices of Google stocks\")\n",
        "\n",
        "# Data Visualization: Plot the difference between high and low prices for Google stocks\n",
        "plx.line(google, x=\"date\", y=[\"high\", \"low\"], title=\"Difference between high and low prices of Google stocks\")\n",
        "\n",
        "# Data Visualization: Plot the volume of Google stocks traded over time\n",
        "plx.line(google, x=\"date\", y=[\"volume\"], title=\"Volume of stock traded\")\n",
        "\n",
        "# Repeat the same visualizations for Facebook stocks\n",
        "facebook = data[data['symbol'] == 'FB']  # Filter the dataset for Facebook stocks\n",
        "plx.line(facebook, x=\"date\", y=[\"open\", \"close\"], title=\"Difference between open and close prices of FB stocks\")\n",
        "plx.line(facebook, x=\"date\", y=[\"high\", \"low\"], title=\"Difference between high and low prices of Facebook stocks\")\n",
        "plx.line(facebook, x=\"date\", y=[\"volume\"], title=\"Volume of stock traded\")\n",
        "\n",
        "# Data Preprocessing: Handle imbalanced data using SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_fb_resampled, y_fb_resampled = smote.fit_resample(X_fb, y_fb)\n",
        "\n",
        "# Convert the resampled data into a DataFrame\n",
        "upsampled_fb = pd.DataFrame(X_fb_resampled, columns=X_fb.columns)\n",
        "upsampled_fb['symbol'] = y_fb_resampled  # Add the 'symbol' column back to the resampled data\n",
        "upsampled_fb.reset_index(drop=True, inplace=True)  # Reset the index of the DataFrame\n",
        "\n",
        "# Randomly assign dates from the original Facebook data to the resampled data\n",
        "upsampled_fb['date'] = np.random.choice(facebook['date'], size=len(upsampled_fb), replace=True)\n",
        "print(upsampled_fb['symbol'].value_counts())  # Print the distribution of symbols after resampling\n",
        "print(upsampled_fb.shape)  # Print the shape of the upsampled data\n",
        "\n",
        "# Prepare Google stock data for time series modeling\n",
        "google = upsampled_fb[upsampled_fb[\"symbol\"] == 'GOOG']\n",
        "google = google.sort_values(by='date')  # Sort the data by date\n",
        "google.reset_index(drop=True, inplace=True)  # Reset the index of the sorted DataFrame\n",
        "\n",
        "# Normalize the 'close' prices using MinMaxScaler\n",
        "close_prices = google['close'].values.reshape(-1, 1)  # Reshape for scaler input\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))  # Initialize scaler with range 0-1\n",
        "close_prices_normalized = scaler.fit_transform(close_prices)  # Normalize the close prices\n",
        "\n",
        "# Define a function to create the dataset for time series forecasting\n",
        "def create_dataset(data, win_size):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - win_size - 1):\n",
        "        X.append(data[i:(i + win_size), 0])  # Extract the features for the window\n",
        "        Y.append(data[i + win_size, 0])  # Extract the target variable\n",
        "    return np.array(X), np.array(Y)  # Return the feature-target pair arrays\n",
        "\n",
        "# Set the window size for the time series\n",
        "win_size = 20\n",
        "X, Y = create_dataset(close_prices_normalized, win_size)  # Create the dataset\n",
        "X = X.reshape(X.shape[0], 1, X.shape[1])  # Reshape X for GRU input\n",
        "\n",
        "# Split the data into training and testing sets (80-20 split)\n",
        "train_size = int(len(X) * 0.8)\n",
        "train_X, test_X = X[0:train_size], X[train_size:]  # Training features and test features\n",
        "train_Y, test_Y = Y[0:train_size], Y[train_size:]  # Training targets and test targets\n",
        "print(train_Y)  # Print the training target values to inspect\n",
        "\n",
        "# Save the test data for future use\n",
        "import pickle\n",
        "with open('google.pkl', 'wb') as file:\n",
        "    pickle.dump(test_X, file)  # Save the test features to a file\n",
        "\n",
        "# Build the GRU model for predicting stock prices\n",
        "model = Sequential()\n",
        "model.add(GRU(units=50, return_sequences=True, input_shape=(1, win_size)))  # First GRU layer with return sequences\n",
        "model.add(GRU(units=50, return_sequences=True))  # Second GRU layer with return sequences\n",
        "model.add(GRU(units=50))  # Third GRU layer without return sequences\n",
        "model.add(Dense(units=1))  # Dense layer for the output\n",
        "\n",
        "# Define custom metrics for model evaluation\n",
        "def rmae(y_true, y_pred):\n",
        "    return tf.sqrt(tf.reduce_mean(tf.abs(y_pred - y_true)))  # Root Mean Absolute Error\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))  # Root Mean Squared Error\n",
        "\n",
        "# Compile the model with the Adam optimizer and mean squared error loss function\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[rmse, rmae])\n",
        "\n",
        "# Display the model summary to check its architecture\n",
        "model.summary()\n",
        "\n",
        "# Train the model on the training data with 50 epochs and a batch size of 32\n",
        "trainmodel = model.fit(train_X, train_Y, epochs=50, batch_size=32, validation_data=(test_X, test_Y))\n",
        "\n",
        "# Save the trained model to a file\n",
        "model.save(\"google_model.h5\")\n",
        "\n",
        "# Define a function to predict the next few days of stock prices using the trained model\n",
        "def predict_next_days(model, X_test_scaled, scaler, num_days):\n",
        "    predicted = []\n",
        "    input_sequence = X_test_scaled[-1].reshape(1, 1, -1)  # Initialize the input sequence with the last test example\n",
        "    for _ in range(num_days):\n",
        "        next_day_pred = model.predict(input_sequence)  # Predict the next day's price\n",
        "        predicted.append(next_day_pred[0, 0])  # Store the prediction\n",
        "        input_sequence = np.append(input_sequence[:, :, 1:], next_day_pred.reshape(1, 1, 1), axis=2)  # Update the input sequence\n",
        "    pred_price = scaler.inverse_transform(np.array(predicted).reshape(-1, 1))  # Scale back the predictions\n",
        "    return pred_price.flatten()  # Return the predictions as a flattened array\n",
        "\n",
        "# Set the number of days to predict\n",
        "num_days_to_predict = 3\n",
        "pred_price = predict_next_days(model, test_X, scaler, num_days_to_predict)  # Predict the next 3 days of stock prices\n",
        "for i in range(num_days_to_predict):\n",
        "    print(f\"Predicted close price for day {i + 1}: ${pred_price[i]:.2f}\")  # Print the predicted prices\n",
        "\n",
        "# Repeat the above steps for Facebook stock data\n",
        "facebook = upsampled_fb[upsampled_fb['symbol'] == 'FB']  # Filter the dataset for Facebook stocks\n",
        "facebook = facebook.sort_values(by='date')  # Sort the data by date\n",
        "facebook.reset_index(drop=True, inplace=True)  # Reset the index of the sorted DataFrame\n",
        "close_prices = facebook['close'].values.reshape(-1, 1)  # Reshape the close prices\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))  # Initialize the scaler\n",
        "normalized_close_prices = scaler.fit_transform(close_prices)  # Normalize the close prices\n",
        "\n",
        "# Create dataset for Facebook stock\n",
        "def make_dataset(data, win_size):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - win_size):\n",
        "        X.append(data[i:i + win_size, 0])  # Extract features for the window\n",
        "        Y.append(data[i + win_size, 0])  # Extract the target variable\n",
        "    return np.array(X), np.array(Y)  # Return the feature-target pair arrays\n",
        "\n",
        "# Set window size and create dataset\n",
        "win_size = 20\n",
        "X, Y = make_dataset(normalized_close_prices, win_size)  # Create the dataset\n",
        "X = X.reshape(X.shape[0], 1, X.shape[1])  # Reshape X for GRU input\n",
        "\n",
        "# Split the data into training and testing sets (80-20 split)\n",
        "train_size = int(len(X) * 0.80)\n",
        "test_size = len(X) - train_size\n",
        "train_X, test_X = X[:train_size], X[train_size:]  # Training features and test features\n",
        "train_Y, test_Y = Y[:train_size], Y[train_size:]  # Training targets and test targets\n",
        "\n",
        "# Build and compile the model for Facebook stock prediction\n",
        "model = Sequential()\n",
        "model.add(GRU(units=50, return_sequences=True, input_shape=(1, win_size)))  # First GRU layer with return sequences\n",
        "model.add(GRU(units=50, return_sequences=True))  # Second GRU layer with return sequences\n",
        "model.add(GRU(units=50))  # Third GRU layer without return sequences\n",
        "model.add(Dense(units=1))  # Dense layer for the output\n",
        "\n",
        "# Compile the model with the Adam optimizer and mean squared error loss function\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[rmse, rmae])\n",
        "\n",
        "# Train the model on the Facebook data\n",
        "trainmodel = model.fit(train_X, train_Y, epochs=50, batch_size=32, validation_data=(test_X, test_Y))\n",
        "\n",
        "# Save the trained model to a file\n",
        "model.save(\"fb_model.h5\")\n",
        "\n",
        "# Predict the next 3 days of stock prices for Facebook\n",
        "num_days_to_predict = 3\n",
        "pred_price = predict_next_days(model, test_X, scaler, num_days_to_predict)\n",
        "for i in range(num_days_to_predict):\n",
        "    print(f\"Predicted close price for day {i + 1}: ${pred_price[i]:.2f}\")\n",
        "\n",
        "# Save the test data for future use\n",
        "with open('facebook_X_test.pkl', 'wb') as file:\n",
        "    pickle.dump(test_X, file)  # Save the test features to a file\n",
        "\n",
        "# Visualization: Compare evaluation metrics between Google and Facebook models\n",
        "Google = {\n",
        "    'RMSE': 0.1954,\n",
        "    'RMAE': 0.3946,\n",
        "    'LOSS': 0.0396\n",
        "}\n",
        "Facebook = {\n",
        "    'RMSE': 0.1909,\n",
        "    'RMAE': 0.3804,\n",
        "    'LOSS': 0.0401\n",
        "}\n",
        "\n",
        "# Prepare metrics and values for plotting\n",
        "metrics = list(Google.keys())\n",
        "google_values = list(Google.values())\n",
        "facebook_values = list(Facebook.values())\n",
        "\n",
        "# Plot the evaluation metrics for Google and Facebook models side by side\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Google model metrics\n",
        "plt.subplot(1, 2, 1)\n",
        "bars = plt.bar(metrics, google_values, color='b', alpha=0.7)\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Metric Values')\n",
        "plt.title('Evaluation Metrics for Google')\n",
        "for bar, value in zip(bars, google_values):\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# Facebook model metrics\n",
        "plt.subplot(1, 2, 2)\n",
        "bars = plt.bar(metrics, facebook_values, color='r', alpha=0.7)\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Metric Values')\n",
        "plt.title('Evaluation Metrics for Facebook')\n",
        "for bar, value in zip(bars, facebook_values):\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.4f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "plt.show()  # Display the plots\n"
      ],
      "metadata": {
        "id": "ofTu91fo8WeA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}